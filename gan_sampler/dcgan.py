#imports
import tensorflow as tf

#DCGAN training loop    
class DCGAN(tf.keras.Model):
    """ The main DCGAN model inheriting the base tensorflow Model class.
        Takes discriminator and generator models and latent dimensions as compulsory inputs.
        The generator generates random samples from a normal distribution.
        discriminator is then used to classify fake and real samples.

    Args:
        discriminator (tensorflow Model): discriminator model.
        generator (tensorflow Model): generator model.
        syn_data (Union[None, tensorflow.Tensor], optional): Synthetic samples generated by traditional over-sampling techniques. Defaults to None.
        random_noise (float, optional): decides to whether use random noise as generator input. Defaults to False.
        kwargs: use it for passing any parent class parameters.
        
    Methods:
        compile: overrides the class compile method. Takes optimizers for discriminator and generator as argument and compiles them along with the model losses.
        
        metrics: overrides the metrics of the model and returns the custom metrices.
        
        train_step: custom training loop that implements the training of DCGAN.
    """
    
    #initialises discriminator, generator and class variables.
    def __init__(self,
                 discriminator:tf.keras.Model,
                 generator:tf.keras.Model,
                 syn_data=None,
                 random_noise:bool=False,
                 **kwargs):
        
        #class variables
        super(DCGAN,self).__init__(**kwargs)
        self.discriminator=discriminator
        self.generator=generator
        self.syn_data=syn_data
        self.random_noise=random_noise
    
    #compile loss functions and optimizers     
    def compile(self, 
                d_optimizer:tf.keras.optimizers.Optimizer,
                g_optimizer:tf.keras.optimizers.Optimizer):
        """overrides the compile method of the model.

        Args:
            d_optimizer (tensorflow optimizer): discriminator optimizer.
            g_optimizer (tensorflow optimizer): generator optimizer.
        """
        super(DCGAN, self).compile()
        
        #optimizers and loss metrics
        self.loss_fn = tf.losses.BinaryCrossentropy() 
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer 
        self.d_loss_metric = tf.metrics.Mean(name="d_loss") 
        self.g_loss_metric = tf.metrics.Mean(name="g_loss")
        self.total_loss_metric = tf.metrics.Mean(name="total_loss")
    
    #override metric behavior of the model     
    @property
    def metrics(self):
        return [self.d_loss_metric, self.g_loss_metric, self.total_loss_metric]
    
    #custom training loop
    @tf.function(reduce_retracing=True)
    def train_step(self, real_data):
        
        #batch-size 
        batch_size = tf.shape(real_data)[0]
         
        #sample random noise from a normal distribution
        if self.random_noise:
            #latent dim
            latent_dim = tf.shape(real_data)[1]
            random_latent_vectors = tf.random.normal(
                    shape=(batch_size, latent_dim)
            )
        #sample from synthetic data
        else:
            random_latent_vector_indices = tf.random.uniform(
                shape=[batch_size],
                minval=0,
                maxval=tf.shape(self.syn_data)[0],
                dtype=tf.int32
            )
            random_latent_vectors=tf.gather(self.syn_data,indices=random_latent_vector_indices)
        
        #Gradient tape for generator and discriminator
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: 
            generated_data = self.generator(random_latent_vectors, training = True)
            real_predictions = self.discriminator(real_data, training = True)
            fake_predictions = self.discriminator( generated_data, training = True)
            
            #real data and fake data label with noise
            real_labels = tf.ones_like(real_predictions)
            real_noisy_labels = real_labels + 0.1 * tf.random.uniform(tf.shape(real_predictions))
            fake_labels = tf.zeros_like(fake_predictions)
            fake_noisy_labels = fake_labels - 0.1 * tf.random.uniform(tf.shape(fake_predictions))
            
            #Calculate discriminator and generator loss
            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)
            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)
            d_loss = (d_real_loss + d_fake_loss) / 2.0
            g_loss = self.loss_fn(real_labels, fake_predictions)
        
        #discriminator gradients
        discriminator_gradients = disc_tape.gradient(
            d_loss, self.discriminator.trainable_variables
        )
        
        #generator gradients
        generator_gradients = gen_tape.gradient(
            g_loss, self.generator.trainable_variables
        )
        
        #update weights of discriminator and generator
        self.d_optimizer.apply_gradients(
            zip(discriminator_gradients, self.discriminator.trainable_variables)
        )
        
        self.g_optimizer.apply_gradients(
            zip(generator_gradients, self.generator.trainable_variables)
        )
        
        #update discriminator and gradient loss
        self.d_loss_metric.update_state(d_loss)
        self.g_loss_metric.update_state(g_loss)
        
        #total_loss
        total_loss=tf.reduce_sum(tf.abs([g_loss,d_loss]))
        self.total_loss_metric.update_state(total_loss)
        
        return {m.name: m.result() for m in self.metrics}