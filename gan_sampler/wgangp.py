#imports
import tensorflow as tf
from typing import Union


#WGAN-GP model    
class WGANGP(tf.keras.models.Model):
    """ The main WGAN model with gradient penalty inheriting the base tensorflow Model class.
        Takes critic and generator models as compulsory inputs.
        The generator generates random samples from a normal distribution or samples generated by SMOTE, Borderline-SMOTE or ADASYN.
        Critic is used to classify fake and real samples.
        Critic is trained multiple times per training of generator to speedup convergence. 

    Args:
        critic (tensorflow.keras.Model): critic model.
        generator (tensorflow.keras.Model): generator model.
        syn_data (Union[None, tensorflow.Tensor], optional): Synthetic samples generated by traditional over-sampling techniques. Defaults to None.
        critic_steps (int, optional): number of training updates for critic per training of the generator. Defaults to 3.
        gp_weight (float, optional): weight for the gradient penalty. Defaults to 10.0.
        random_noise (float, optional): decides to whether use random noise as generator input. Defaults to False.
        kwargs: use it for passing any parent class parameters.
        
    Methods:
        compile: overrides the class compile method. Takes optimizers for critic and generator as argument and compiles them along with the model losses.
        
        metrics: overrides the metrics of the model and returns the custom metrices.
        
        gradient_penalty: calculate the gradient penalty using real and fake samples.
        
        train_step: custom training loop that implements the training of WGANGP.
    """
    #initialises critic, generator and class variables.
    def __init__(self,
                 critic:tf.keras.Model,
                 generator:tf.keras.Model,
                 syn_data:Union[None,tf.Tensor]=None,
                 critic_steps:int=3,
                 gp_weight:float=10.0,
                 random_noise:bool=False,
                 **kwargs):
        
        super(WGANGP,self).__init__(**kwargs)
        #class variables
        self.critic = critic
        self.generator = generator
        self.syn_data=syn_data
        self.critic_steps = critic_steps
        self.gp_weight = gp_weight
        self.random_noise=random_noise
    
    #compile loss functions and optimizers  
    def compile(self,
                c_optimizer:tf.keras.optimizers.Optimizer,
                g_optimizer:tf.keras.optimizers.Optimizer):
        """overrides the compile method of the model.

        Args:
            c_optimizer (tensorflow optimizer): critic optimizer.
            g_optimizer (tensorflow optimizer): generator optimizer.
        """
        super(WGANGP, self).compile()
        
        #optimizers and loss metrics
        self.c_optimizer = c_optimizer
        self.g_optimizer = g_optimizer
        self.c_wass_loss_metric = tf.keras.metrics.Mean(name="c_wass_loss")
        self.c_gp_metric = tf.keras.metrics.Mean(name="c_gp")
        self.c_loss_metric = tf.keras.metrics.Mean(name="c_loss")
        self.g_loss_metric = tf.keras.metrics.Mean(name="g_loss")
        self.total_loss_metric = tf.keras.metrics.Mean(name="total_loss")
    
    #override metric behavior of the model    
    @property
    def metrics(self):
        return [
            self.c_loss_metric,
            self.c_wass_loss_metric,
            self.c_gp_metric,
            self.g_loss_metric,
            self.total_loss_metric
        ]
    
    #gradient penalty
    @tf.function(reduce_retracing=True)  
    def gradient_penalty(self, batch_size:int, real_data, fake_data):
        """calculates the gradient penalty

        Args:
            batch_size (int): batch size.
            real_data (Tensor): real data tensor.
            fake_data (Tensor): fake data generated by the generator. 

        Returns:
            float: gradient penalty
        """
        alpha = tf.random.normal([batch_size, real_data.shape[1]],0.0,1.0)
        diff = fake_data - real_data
        interpolated = real_data + alpha * diff
        with tf.GradientTape() as gp_tape:
            gp_tape.watch(interpolated)
            pred = self.critic(interpolated, training=True)

        grads = gp_tape.gradient(pred, [interpolated])[0]
        
        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1)+ 1e-12)
        gp = tf.reduce_mean((norm - 1.0) ** 2)
        return gp
    
    #custom training loop
    @tf.function(reduce_retracing=True)
    def train_step(self, real_data):
        
        #batch-size
        batch_size = tf.shape(real_data)[0]
        
        #train the critic multiple times and update the weights
        for _ in tf.range(0,self.critic_steps):
            
            #sample random noise from a normal distribution
            if self.random_noise:
                #latent dim
                latent_dim = tf.shape(real_data)[1]
                random_latent_vectors = tf.random.normal(
                        shape=(batch_size, latent_dim)
                )
            #sample from synthetic data
            else:
                random_latent_vector_indices = tf.random.uniform(
                    shape=[batch_size],
                    minval=0,
                    maxval=tf.shape(self.syn_data)[0],
                    dtype=tf.int32
                )
                random_latent_vectors=tf.gather(self.syn_data,indices=random_latent_vector_indices)
            #calculate gradients of critic
            with tf.GradientTape() as tape:
                fake_data_gen = self.generator(
                    random_latent_vectors, training=True
                )
                #fake and real label prediction
                fake_predictions = self.critic(fake_data_gen, training=True)
                real_predictions = self.critic(real_data, training=True)

                #calculate the losses
                c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(
                    real_predictions
                )
                #gradient penalty
                c_gp = self.gradient_penalty(
                    batch_size, real_data, fake_data_gen
                )
                #total critic loss
                c_loss = c_wass_loss + c_gp * self.gp_weight
                
            #calculate the gradients    
            c_gradient = tape.gradient(c_loss, self.critic.trainable_variables)
            
            
            #apply the gradients
            self.c_optimizer.apply_gradients(
                zip(c_gradient, self.critic.trainable_variables)
            )

        #train the generator and update the weights
        #sample random noise from a normal distribution
        if self.random_noise:
            #latent dim
            latent_dim = tf.shape(real_data)[1]
            random_latent_vectors = tf.random.normal(
                    shape=(batch_size, latent_dim)
            )
        #sample from synthetic data
        else:
            random_latent_vector_indices = tf.random.uniform(
                shape=[batch_size],
                minval=0,
                maxval=tf.shape(self.syn_data)[0],
                dtype=tf.int32
            )
            random_latent_vectors=tf.gather(self.syn_data,indices=random_latent_vector_indices)
            
        with tf.GradientTape() as tape:
            fake_data_gen = self.generator(random_latent_vectors, training=True)
            fake_predictions = self.critic(fake_data_gen, training=True)
            
            #generator loss is the negative of fake prediction loss
            g_loss = -tf.reduce_mean(fake_predictions)

        #calculate the generator gradients
        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)
        
        
        #apply the gradients
        self.g_optimizer.apply_gradients(
            zip(gen_gradient, self.generator.trainable_variables)
        )

        #update loss metrics
        self.c_loss_metric.update_state(c_loss)
        self.c_wass_loss_metric.update_state(c_wass_loss)
        self.c_gp_metric.update_state(c_gp)
        self.g_loss_metric.update_state(g_loss)
        
        #update total loss
        total_loss = tf.reduce_sum(tf.abs([c_loss,c_wass_loss,c_gp,g_loss]))
        self.total_loss_metric.update_state(total_loss)
        
        return {m.name: m.result() for m in self.metrics}